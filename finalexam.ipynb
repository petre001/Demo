{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5dc18aa3b9a75dbea709feb911a8cd68",
     "grade": false,
     "grade_id": "cell-22d0f94f6aa32bda",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e9dd77cacc640add2a085a2cac7e56bd",
     "grade": false,
     "grade_id": "cell-f7f4291b2367dcbf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Predicting Demand for Washington D.C.'s Bike Share System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ddf0e662a0f83db0af01fe78a21a5ab3",
     "grade": false,
     "grade_id": "cell-72ed6de038e2c607",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Background\n",
    "You have just been hired as the first data scientist working for Capital Bikeshare, the organization which runs the Washington D.C. bike sharing system. The first major project they have asked you to work on is to build a model to predict demand for the shared bikes in the system for each hour of each day.  \n",
    "\n",
    "Having an accurate understanding of the expected demand is critical to the successful operation of Capital Bikeshare.  If they underestimate demand and have too few bikes available, potential users of the system are not able to find a bike to use and so get upset and are less likely to use the system in the future.  If they overestimate by too much, they end up with too many bikes sitting around not being used.  In the real-world, one of the things that makes this challenging is that they have to predict demand for each pick-up hub location.  To keep things simple for our final, we will focus on predicting aggregate demand.\n",
    "\n",
    "Our task in this final is to build the pipeline to convert raw data into features to use in a ML model. The model itself that you will use has already been set up for you (a linear regression model) and **you cannot change the model**, only the data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c8c3f2e58cc71c13ee1dfb052b8a797",
     "grade": false,
     "grade_id": "cell-a529fa2c32818314",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Data\n",
    "You have been given two csv files of data to use in your analysis.  The first file (\"2011-2012_bikes.csv\") contains historical demand data from the past two years of operation. The dataset contains the following columns:\n",
    "- dteday : date \n",
    "- hr : hour (0 to 23) \n",
    "- cnt: count of total rental bikes \n",
    "\n",
    "The second file (\"2011-2012_weather_messy.csv\") contains weather information for the same time period.  This dataset contains the following columns:  \n",
    "- dteday : date \n",
    "- hr : hour (0 to 23) \n",
    "- weathersit : \n",
    "    - 1: Clear, Few clouds, Partly cloudy, Partly cloudy \n",
    "    - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist \n",
    "    - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds \n",
    "    - 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n",
    "- temp : Temperature in Celsius\n",
    "- atemp: Feels-like temperature in Celsius\n",
    "- hum: Humidity\n",
    "- windspeed: Wind speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "848275acd4946e512675f3de6a8f3a54",
     "grade": false,
     "grade_id": "cell-caaa3b69ecda13df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this before any other code cell\n",
    "# This downloads the csv data files into the same directory where you have saved this notebook\n",
    "\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "import os\n",
    "path = Path()\n",
    "\n",
    "# Dictionary of file names and download links\n",
    "files = {'2011-2012_bikes.csv':'https://storage.googleapis.com/aipi_datasets/2011-2012_bikes.csv',\n",
    "        '2011-2012_weather_messy.csv': 'https://storage.googleapis.com/aipi_datasets/2011-2012_weather_messy.csv'}\n",
    "\n",
    "# Download each file\n",
    "for key,value in files.items():\n",
    "    filename = path/key\n",
    "    url = value\n",
    "    # If the file does not already exist in the directory, download it\n",
    "    if not os.path.exists(filename):\n",
    "        urllib.request.urlretrieve(url,filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "62ff28b8e2bb07eefab73400d7e402af",
     "grade": false,
     "grade_id": "cell-6b144c6829115d5d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**IMPORTANT NOTE**: For this exercise you may use ONLY the packages which are imported for you below.  You may not import any additional libraries yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6648b63dba2e8e7484bcffdb89f86b5b",
     "grade": false,
     "grade_id": "cell-81357d8c0527b793",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
    "\n",
    "\n",
    "# This imports the run_model function from the model.py script provided to you - must be in same directory\n",
    "from model import run_model\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba98a06ffa123872d1271b15286dc888",
     "grade": false,
     "grade_id": "cell-efcc307a64ed278a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question 1\n",
    "Complete the below function `load_data()` which takes the bike usage data and the weather data file as inputs and does the following:  \n",
    "- Join the weather data onto the bike data dataframe using a \"left\" merge on the common keys (hint: you may need to use two common keys to properly merge the files). \n",
    "- Set the index of the merged dataframe to a pandas DatetimeIndex that includes BOTH date and time (hour of day). \n",
    "\n",
    "Your function should return a dataframe with the DatetimeIndex index and containing the following columns:  \"cnt\", \"weathersit\", \"temp\", \"atemp\", \"hum\", \"windspeed\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "15a2ee89d3a919efbdacc74a6f3ebab8",
     "grade": false,
     "grade_id": "cell-b80b9bb85d4194ca",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def load_data(bike_data, weather_data):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0cff1204cd3231e8885e706887c86b23",
     "grade": true,
     "grade_id": "cell-b218bd5fac9f45e5",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell for load_data\n",
    "bike_datafile = \"2011-2012_bikes.csv\"\n",
    "weather_datafile = \"2011-2012_weather_messy.csv\"\n",
    "bike_data = pd.read_csv(bike_datafile)\n",
    "weather_data = pd.read_csv(weather_datafile)\n",
    "    \n",
    "df = load_data(bike_data,weather_data)\n",
    "display(df.head())\n",
    "assert df.shape == (17379,6)\n",
    "assert type(df.index)==pd.core.indexes.datetimes.DatetimeIndex\n",
    "assert set(df.columns) == set(['cnt','weathersit','temp','atemp','hum','windspeed'])\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "df['cnt'].plot()\n",
    "plt.title('Demand for bikes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed256665853059bd7f3a0a7673dfe00c",
     "grade": false,
     "grade_id": "cell-39364e90187f09b0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that we've got our data loaded and organized, let's do some clean up.  As we can see when we run the cell below, we have some missing values that we will need to fill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea4f02fa60a32630efd8b378512eb782",
     "grade": false,
     "grade_id": "cell-1f58865370227edb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "372642f020b533ee1ea162e9d4b1b654",
     "grade": false,
     "grade_id": "cell-66c03a03105ee793",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question 2\n",
    "Complete the below function `fill_missing()` which fills in the missing data in our dataframe (all columns) using a forward-fill method from the last previously recorded values of each feature, and then returns the dataframe with all missing values filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "afd119322d27aad0c6c3ba387fcf2fa6",
     "grade": false,
     "grade_id": "cell-5c4638c5ecf84143",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def fill_missing(data):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "00dd7b17566975f9bfa9954b1b823950",
     "grade": true,
     "grade_id": "cell-ea7d1d42fdd9f5ca",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell for remove_nulls()\n",
    "df_clean = fill_missing(df.copy())\n",
    "\n",
    "assert df_clean.isna().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c09751662d7d4dd83435f51b1dadedc8",
     "grade": false,
     "grade_id": "cell-22fe2d558731fba8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question 3\n",
    "We now need to identify and deal with any outliers in our data.  Complete the below function `correct_outliers()` which takes the following inputs:  \n",
    "- the dataframe output from `fill_missing`. \n",
    "- a list of columns to check for outliers (`cols_to_check`), which will contain our continuous features. \n",
    "- the `k` value to use in the IQR method to check for outliers  \n",
    "\n",
    "Your function should use the IQR method we covered in class to check for outliers above the upper threshold (`q75 + k * iqr`) or below the lower threshold (`q25 - k * iqr`).  You should replace the outliers with the last previously recorded value from the columns which is not an outlier (Hint: one way to do this is to replace the outlier values you find with `None` and then apply your `fill_missing()` function from above again to fill the gaps).\n",
    "\n",
    "The function should return the dataframe with the outlier values replaced by the last previously recorded values which are not outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d69d5a4a8d38529d6259a2377b084486",
     "grade": false,
     "grade_id": "cell-2631d0d099d76866",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def correct_outliers(data,cols_to_check,k):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "26bb593849a92eed841a01531bb84741",
     "grade": true,
     "grade_id": "cell-90cad10171e33c62",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell for correct_outliers()\n",
    "cols = ['temp','atemp','hum','windspeed']\n",
    "df_no_outliers = correct_outliers(df_clean.copy(),cols,3)\n",
    "\n",
    "a = np.std(df_clean,axis=0)\n",
    "b = np.std(df_no_outliers,axis=0)\n",
    "count = 0\n",
    "for col in cols:\n",
    "    if b[col] < a[col]:\n",
    "        count+=1\n",
    "assert count == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f4961d25c2e53f0f3f8e005c98d8432",
     "grade": false,
     "grade_id": "cell-f15ec71cf3a1a85a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question 4\n",
    "Now that our data is cleaned up, let's split our data into training and test sets to use in modeling.\n",
    "\n",
    "NOTE: normally on a real problem we would do this first before handling outliers.  We would then build our method for identifying and handling outliers using the training set only, and apply it to both the training and test sets.  But to keep things simple for this exam we have dealt with outliers first in the entire dataset and then split our data.\n",
    "\n",
    "Complete the below function `split_data()` which splits the data into the training datasets `X_train, y_train` and the test datasets `X_test, y_test`.  Since this is a time series problem, instead of randomly splitting the data your function should use all data up to and including July 31 2012 as the training set and the data for the period August 1 2012 - December 31 2012 as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c8bcd56e72550acaf0d10541dd0ad95",
     "grade": false,
     "grade_id": "cell-578054bc7e2183d6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1baa5b21578292f69f7af90d40528b2",
     "grade": true,
     "grade_id": "cell-a453825f46181d88",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell for split_data()\n",
    "X_train,y_train,X_test,y_test = split_data(df_no_outliers.copy())\n",
    "assert X_train.shape == (13747,5)\n",
    "assert len(y_train) == 13747\n",
    "assert X_test.shape == (3632,5)\n",
    "assert len(y_test) == 3632"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23e74fb67e88e0b3df88b244c5cc20be",
     "grade": false,
     "grade_id": "cell-0e21b678384ebeb2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question 5\n",
    "Let's now create a few new features to account for possible seasonality/cyclicality in demand for bicycle rentals.  Complete the below function `create_timeseries_features()` which takes a dataframe of input features (`X_train` and then `X_test`) as input and adds the following features to it (named exactly as stated below):  \n",
    "- 'hourofday': hour of day, 0 to 23. \n",
    "- 'dayofweek': day of week as an integer, 0 to 6. \n",
    "- 'month': month of year as an integer, 0 to 11. \n",
    "- 'year': year as an integer\n",
    "\n",
    "Your function should return the input feature dataframe with the original columns and the the above features added, named exactly as stated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "122d23d7f4cde776a7a44f903cb7d5da",
     "grade": false,
     "grade_id": "cell-dc5ebcdf542217d6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_timeseries_features(X):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "47dad9fe03233946873f657a9f99fe7a",
     "grade": true,
     "grade_id": "cell-24a7b85f24d81ea1",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell for create_timeseries_features()\n",
    "X_train_full = create_timeseries_features(X_train.copy())\n",
    "X_test_full = create_timeseries_features(X_test.copy())\n",
    "display(X_train.head())\n",
    "\n",
    "assert set(X_train_full.columns) == set(['weathersit','temp','atemp','hum','windspeed','hourofday','dayofweek','month','year'])\n",
    "assert X_train_full['hourofday'].nunique()==24\n",
    "assert X_train_full['dayofweek'].nunique()==7\n",
    "assert X_train_full['month'].nunique()==12\n",
    "assert X_train_full['year'].nunique()==2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c9afd2a35a57b3a945a041333af26d3",
     "grade": false,
     "grade_id": "cell-c341995cddfacb2b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question 6\n",
    "Let's now evaluate whether our continuous weather features have any value in predicting bike demand.  Complete the below function `cont_feat_select()` which takes the training data (X_train_full,y_train) and the list of continuous features as input.  The function should perform a univariate test on the features listed in the input list `feats` using `SelectKBest`.  You should use `f_regression` as the scoring function (which evaluates features based on correlation with the numerical target).  Your function should create and return a dataframe containing two columns:   \n",
    "- `Feature` should contain the name of each continuous feature  \n",
    "- `Score` should contain each feature's corresponding f-score on the univariate test\n",
    "\n",
    "The rows of your dataframe should be sorted by `Score` with the features with the largest scores first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d437edbd9be923b1aafc54bd57801d8",
     "grade": false,
     "grade_id": "cell-0d3a200b6fa58a74",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def cont_feat_select(X,y,feats):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dac6f1509ffeb39435cfd5fe211c022d",
     "grade": true,
     "grade_id": "cell-8ba6041bf2e6b88b",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell for cont_feat_select()\n",
    "cont_feats = ['temp','atemp','hum','windspeed']\n",
    "f_scores = cont_feat_select(X_train_full,y_train,cont_feats)\n",
    "display(f_scores)\n",
    "\n",
    "assert f_scores.shape==(4,2)\n",
    "for col in ['Feature','Score']:\n",
    "    assert col in f_scores.columns\n",
    "    \n",
    "assert f_scores['Score'][0]>=f_scores['Score'][1]\n",
    "\n",
    "# Plot scores\n",
    "plt.bar(x=f_scores['Feature'],height=f_scores['Score'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d3bdc8e31eb559da8695e27e3c18dc8",
     "grade": false,
     "grade_id": "cell-0268b1ac713a4ab7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question 7\n",
    "Let's now drop the lowest scoring continuous feature, based on our univariate test above.  Complete the below function `drop_lowest_feat()` which takes as input the X_train_full and X_test_full dataframes and your feature scoring dataframe output from your `cont_feat_select()` function above.  Your function should identify the lowest performing feature from your `f_scores` dataframe and drop it from both X_train_full and X_test_full.  It should then return the X_train_full and X_test_full dataframes with the feature dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "934304e783b8957307a82ec1535b27d0",
     "grade": false,
     "grade_id": "cell-57a2333ae045134d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def drop_lowest_feat(X_train,X_test,scores_df):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cfdaff7b34d2eabda4cefcad2f554f3d",
     "grade": true,
     "grade_id": "cell-e6588a9fae14d19f",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell for drop_lowest_feat()\n",
    "X_train_reduced,X_test_reduced = drop_lowest_feat(X_train_full.copy(),X_test_full.copy(),f_scores)\n",
    "display(X_train_reduced.head())\n",
    "\n",
    "assert X_train_reduced.shape==(13747, 8)\n",
    "assert X_test_reduced.shape==(3632, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2479c4206e557a9a3c2b59a0f429c797",
     "grade": false,
     "grade_id": "cell-09ef0b02826676d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question 8\n",
    "We now need to encode our categorical features to prepare them for use in modeling.  Complete the below function `encode_training()` which takes the training data `X_train_reduced` and the list of categorical features (`catfeats`) as inputs. The function should use `OneHotEncoder(handle_unknown='ignore')` to one-hot encode the categorical features in the training dataframe (make sure to drop the original categorical columns after one-hot encoding them).  Your function should return the `X_train_reduced` dataframe with the categorical columns listed in `catfeats` one-hot encoded, and the one-hot encoder object (which we will use later to encode the test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8cad32768f26b84a5f9177de89147176",
     "grade": false,
     "grade_id": "cell-ff64eead1e1344c4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def encode_training(X,catfeats):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ccff64afc532460c23dc816834c6d512",
     "grade": true,
     "grade_id": "cell-2b5d37fce0d63410",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell for encode_training()\n",
    "cat_feats = ['hourofday','weathersit','dayofweek','month','year']\n",
    "\n",
    "X_train_encoded,onehot_enc = encode_training(X_train_reduced.copy(),cat_feats)\n",
    "\n",
    "for col in cat_feats:\n",
    "    assert col not in X_train_encoded.columns\n",
    "    \n",
    "assert X_train_encoded.shape==(13747,52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b3d3e05e0653afc0a1e22fcc8b65383",
     "grade": false,
     "grade_id": "cell-051c3772cf091103",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question 8\n",
    "We can now use our encoder to encode the test set.  Complete the below function `encode_test()` which takes the test set `X_test_reduced`, the list of categorical features (`catfeats`) and the one-hot encoder object `onehot_enc` as inputs.  The function should use the encoder to one-hot encode X_test (be sure to drop the original categorical columns from the dataframe after one-hot encoding them).  Your function should return the X_test dataframe with ther categorical columns one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65b853881d9a4063eae8b738904da16f",
     "grade": false,
     "grade_id": "cell-fb2f6842a35928a4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def encode_test(X,catfeats,encoder):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "877db55c0971066cda6efa056f23b39c",
     "grade": true,
     "grade_id": "cell-6141d6dd569ca573",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell for encode_test()\n",
    "X_test_encoded = encode_test(X_test_reduced.copy(),cat_feats,onehot_enc)\n",
    "    \n",
    "for col in cat_feats:\n",
    "    assert col not in X_test_encoded.columns\n",
    "    \n",
    "assert X_test_encoded.shape==(3632,52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b8e61585e2fd5d0e7787c25356e51f16",
     "grade": false,
     "grade_id": "cell-cd9d8e5013253357",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question 9\n",
    "Finally, let's standardize our continuous features before we run our model.  Complete the below function `scale_cont_feats()` which takes the X_train and X_test dataframes as input and a list of the continuous features (`feats`) to standardize.  Your function should use `StandardScaler()` to standardize the continuous features (or do so manually if you prefer) and return the X_train and X_test dataframes with the continuous features (listed in the `feats` input list) standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5fd6adebeb94686fbdfdcf4d91e6e40",
     "grade": false,
     "grade_id": "cell-f914f1eb4902eff1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def scale_cont_feats(X_train,X_test,feats):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a60f85beba0c5e57625c6767886bb3c",
     "grade": true,
     "grade_id": "cell-25b8ae028a6a725c",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell for scale_cont_feats\n",
    "feats = [col for col in list(X_train_reduced.columns) if col not in cat_feats]\n",
    "X_train_final,X_test_final = scale_cont_feats(X_train_encoded.copy(),X_test_encoded.copy(),feats)\n",
    "\n",
    "for feat in feats:\n",
    "    assert np.round(np.mean(X_train_final.loc[:,feat]),1)==0\n",
    "    assert np.round(np.mean(X_test_final.loc[:,feat]),5)!=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7bfad361d7adca2ac2a121aa3bc126a9",
     "grade": false,
     "grade_id": "cell-278cf4c4ab73ce46",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that we've prepared our features we are ready to run our model.  Run the cell below, which trains the model on the training set and calculates and reports the mean squared error (MSE) on the test set.  If everything went well you should have a MSE below 18500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f1ea6a44016199fa958f9f03ef0f9346",
     "grade": false,
     "grade_id": "cell-1d84dda8b73e82cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mse_score = run_model(X_train_final,y_train,X_test_final,y_test)\n",
    "print('Mean Squared Error on the test set: {:.2f}'.format(mse_score))\n",
    "\n",
    "assert mse_score < 18500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cc1566edea671102fd232aacafeca559",
     "grade": false,
     "grade_id": "cell-19b4a654f5b85616",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### The End\n",
    "Congratulations on finishing the exam, and we hope you enjoyed the course!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
